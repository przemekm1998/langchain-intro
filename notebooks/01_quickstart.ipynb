{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:09:07.185298Z",
     "start_time": "2023-11-30T16:09:07.164503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.\n",
    "\n",
    "PromptTemplates help with bundling up all the logic for going from user input into a fully formatted prompt.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de44b95b3621e4df"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'Tell me a joke about programming'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm_prompt = llm_template.format(topic=\"programming\")\n",
    "llm_prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:10:03.730524Z",
     "start_time": "2023-11-30T16:10:03.714657Z"
    }
   },
   "id": "853a06573352ef93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "PromptTemplates can produce a list of messages. The prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). Here, what happens most often is a `ChatPromptTemplate` is a list of `ChatMessageTemplates.` Each `ChatMessageTemplate` contains instructions for how to format that `ChatMessage` - its role, and then also its content."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc787c7cf88b87d7"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[SystemMessage(content='You are a helpful assistant that tells jokes about topic given by human. Tell the jokes in Polish language.'),\n HumanMessage(content='I love programming.')]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "system_template = \"You are a helpful assistant that tells jokes about topic given by human. Tell the jokes in {joke_language} language.\"\n",
    "human_template = \"{human_text}\"\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chat_prompt = chat_template.format_messages(joke_language=\"Polish\", human_text=\"I love programming.\")\n",
    "chat_prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:11:01.324702Z",
     "start_time": "2023-11-30T16:11:01.312796Z"
    }
   },
   "id": "dd24ca0f8c8dd32f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM / Chat model\n",
    "\n",
    "There are two types of language models:\n",
    "\n",
    "`LLM`: underlying model takes a string as input and returns a string\n",
    "`ChatModel`: underlying model takes a list of messages as input and returns a message"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "286e8143c12010ec"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n\\nQ: Why did the programmer quit his job?\\nA: Because he didn't get arrays.\""
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "llm.invoke(llm_prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:12:14.803141Z",
     "start_time": "2023-11-30T16:12:13.368226Z"
    }
   },
   "id": "d63750ec5708e56"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='Świetnie! To mam dla Ciebie żart programistyczny:\\n\\nProgramista wraca z zakupów do domu i mówi żonie: \"Kochanie, kupiłem piwo, bo wiem, że to Twoja ulubiona metoda sortowania!\"\\n\\nŻona na to: \"A gdzie jest reszta zakupów?\"\\n\\nProgramista odpowiada: \"No wiesz, sortowanie bąbelkowe jest dość wolne...\"')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "chat_model.invoke(chat_prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:12:43.621971Z",
     "start_time": "2023-11-30T16:12:40.025503Z"
    }
   },
   "id": "5522f51c2555110d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The base message interface is defined by BaseMessage, which has two required attributes:\n",
    "\n",
    "- `content`: The content of the message. Usually a string.\n",
    "- `role`: The entity from which the BaseMessage is coming.\n",
    "\n",
    "\n",
    "LangChain provides several objects to easily distinguish between different roles:\n",
    "\n",
    "- `HumanMessage`: A BaseMessage coming from a human/user.\n",
    "- `AIMessage`: A BaseMessage coming from an AI/assistant.\n",
    "- `SystemMessage`: A BaseMessage coming from the system.\n",
    "- `FunctionMessage / ToolMessage`: A BaseMessage containing the output of a function or tool call. (*)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53a92825e5f23cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output parsers\n",
    "\n",
    "OutputParsers convert the raw output of a language model into a format that can be used downstream."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6248ea021f434d76"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "[SystemMessage(content='You are a helpful assistant that tells jokes about topic given by human. Tell the jokes in Polish language. The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"description\": \"question to set up a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'),\n HumanMessage(content='I love programming.')]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "system_template = \"You are a helpful assistant that tells jokes about topic given by human. Tell the jokes in {joke_language} language. {format_instructions}\"\n",
    "human_template = \"{human_text}\"\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "chat_prompt = chat_template.format_messages(joke_language=\"Polish\", human_text=\"I love programming.\", format_instructions=parser.get_format_instructions())\n",
    "chat_prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:15:11.077453Z",
     "start_time": "2023-11-30T16:15:11.041811Z"
    }
   },
   "id": "9f9ddb000cfd1d20"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Joke(setup='Dlaczego programiści lubią pływać?', punchline='Bo w wodzie czują się jak w float')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_output = chat_model.invoke(chat_prompt)\n",
    "parser.invoke(raw_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:16:04.744140Z",
     "start_time": "2023-11-30T16:16:03.144080Z"
    }
   },
   "id": "7b50feca2eb0d277"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "25b61afe40724efb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
